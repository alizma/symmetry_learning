{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# includes residual\n",
    "       \n",
    "class Lconv_core(nn.Module):\n",
    "    \"\"\" L-conv layer with full L \"\"\"\n",
    "    def __init__(self,d,num_L=1,cin=1,cout=1,rank=8):\n",
    "        \"\"\"\n",
    "        L:(num_L, d, d)\n",
    "        Wi: (num_L, cout, cin)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.L = nn.Parameter(torch.Tensor(num_L, d, d))\n",
    "        self.Wi = nn.Parameter(torch.Tensor(num_L, cout, cin))\n",
    "        \n",
    "        # initialize weights and biases\n",
    "        nn.init.kaiming_normal_(self.L) \n",
    "        nn.init.kaiming_normal_(self.Wi)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # x:(batch, channel, flat_d)\n",
    "        # h = (x + Li x Wi) W0\n",
    "        y = torch.einsum('kdf,bcf,koc->bod', self.L, x, self.Wi ) +x #+ self.b        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,d,c,device,k=1, rec=1, hid = 5):\n",
    "        super().__init__()\n",
    "        self._rec = rec\n",
    "        self._c = c\n",
    "#         with torch.no_grad():\n",
    "        self.ones = torch.ones((1,c,1)).to(device)\n",
    "        self.LC = Lconv_core(d=d,num_L=k,cin=c,cout=c)\n",
    "        self.flat = nn.Flatten(2)\n",
    "        self.lin1 = nn.Linear(c,hid)\n",
    "        self.out = nn.Linear(hid,1)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        print(inp.shape)\n",
    "        x,y = inp\n",
    "        # x,y->(batch, channel, flat_d)\n",
    "        x = self.flat(x)\n",
    "        y = self.flat(y)\n",
    "        # copy input to c channels\n",
    "        x = x * self.ones\n",
    "        # pass through L-conv\n",
    "        for _ in range(self._rec):\n",
    "            x = self.LC(x)\n",
    "        x = torch.einsum('bcd,bad->bc', x, y)\n",
    "        x = F.tanh(x)\n",
    "        x = self.lin1(x)\n",
    "        x = F.tanh(x) \n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self,shape=None):\n",
    "        self.shape = shape\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return x.view(-1,*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "class trainer():\n",
    "    def __init__(self, model, device, optimizer, dataset_class = None, \n",
    "                 train_loader=None, test_loader=None,\n",
    "                 batch_size = 64, test_batch_size = 1000,loss_func = F.nll_loss,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        usage:\n",
    "            t = trainer(...)\n",
    "            t.fit(epochs)\n",
    "            \n",
    "        methods:\n",
    "            .fit(epochs) : train + test; print results; stores results in trainer.history <dict>\n",
    "            .train(epoch)\n",
    "            .test()\n",
    "        \"\"\"\n",
    "        self.device = device #torch.device(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        self.scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "        self.history = {'train loss':[], 'test loss':[], 'train acc':[], 'test acc':[], 'train time':[]}\n",
    "        \n",
    "        if dataset_class:\n",
    "            self.make_dataloaders(dataset_class, batch_size, test_batch_size)\n",
    "        else:\n",
    "            self.train_loader = train_loader \n",
    "            self.test_loader = test_loader\n",
    "        \n",
    "    def make_dataloaders(self, dataset_class, batch_size, test_batch_size):\n",
    "        train_kwargs = {'batch_size': batch_size}\n",
    "        test_kwargs = {'batch_size': test_batch_size}\n",
    "        if self.device.type =='cuda':        \n",
    "            cuda_kwargs = {'num_workers': 1,\n",
    "                           'pin_memory': True,\n",
    "                           'shuffle': True}\n",
    "            train_kwargs.update(cuda_kwargs)\n",
    "            test_kwargs.update(cuda_kwargs)\n",
    "        \n",
    "        print('Creating data loaders...',end='')\n",
    "        dataset1 = dataset_class('../data', train=True, download=True,)\n",
    "        dataset2 = dataset_class('../data', train=False,)\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "        self.test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "        print('Done')\n",
    "\n",
    "#         self.optimizer = optimizer\n",
    "#         self.model, self.device, self.train_loader,  = model, device, train_loader,\n",
    "#         self.test_loader = test_loader\n",
    "        \n",
    "    def progbar(self,percent, N=10):\n",
    "        n = int(percent//N)\n",
    "        return '[' + '='*n + '>' +'.'*(N-n-1) +']'\n",
    "    \n",
    "    def train(self,epoch):\n",
    "        self.model.train()\n",
    "        training_loss = 0\n",
    "        correct = 0\n",
    "        t0 = time()\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            if type(data)==list:\n",
    "                data = [d.to(self.device) for d in data]\n",
    "            else:\n",
    "                data = data.to(self.device)\n",
    "            if type(target)==list:\n",
    "                target = [d.to(self.device) for d in target]\n",
    "            else:\n",
    "                target = target.to(self.device)\n",
    "                \n",
    "#             data, target = data.to(self.device), target.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            print(self.model(data).shape)\n",
    "            output = self.model(data)\n",
    "            loss = self.loss_func(output, target)\n",
    "            training_loss += loss.sum().item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                perc = 100. * batch_idx / len(self.train_loader)\n",
    "                t1 = time()\n",
    "                print('Train Epoch: {} {} {:.1f}s [{}/{} ({:.0f}%)]\\tLoss: {:.4g}'.format(\n",
    "                    epoch, self.progbar(perc), t1-t0,\n",
    "                    batch_idx * len(data), len(self.train_loader.dataset), # n/N\n",
    "                    perc, # % passed\n",
    "                    loss.item()), end='\\r')\n",
    "        \n",
    "        training_loss /= len(self.train_loader.dataset)\n",
    "        acc = correct / len(self.train_loader.dataset)    \n",
    "        print('\\nTraining: loss: {:.4g}, Acc: {:.2f}%'.format(training_loss, 100.*acc))\n",
    "            \n",
    "        return {'loss':training_loss, 'acc':acc , 'time':t1-t0}\n",
    "                \n",
    "        \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in self.test_loader:\n",
    "                if type(data)==list:\n",
    "                    data = [d.to(self.device) for d in data]\n",
    "                else:\n",
    "                    data = data.to(self.device)\n",
    "                if type(target)==list:\n",
    "                    target = [d.to(self.device) for d in target]\n",
    "                else:\n",
    "                    target = target.to(self.device)\n",
    "                #data, target = data.to(self.device), target.to(self.device)\n",
    "                print(self.model(data).shape)\n",
    "                output = self.model(data)\n",
    "                \n",
    "                test_loss += self.loss_func(output, target, reduction='sum').item()  # sum up batch loss\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        \n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        test_acc = correct / len(self.test_loader.dataset)\n",
    "        \n",
    "        print('Test loss: {:.4g}, Test acc.: {:.2f}%'.format( test_loss, 100.*test_acc))\n",
    "        return {'loss':test_loss, 'acc':test_acc}\n",
    "    \n",
    "    def fit(self,epochs=1):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            r = self.train(epoch)\n",
    "            self.history['train loss'] += [r['loss']]\n",
    "            self.history['train acc'] += [r['acc']]\n",
    "            self.history['train time'] += [r['time']]\n",
    "            \n",
    "            r = self.test()\n",
    "            self.history['test loss'] += [r['loss']]\n",
    "            self.history['test acc'] += [r['acc']]\n",
    "            self.scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "DIM = 20 \n",
    "\n",
    "# TODO rather than  generating a bunch of\n",
    "# small perturbed \n",
    "class FixedRot(datasets.VisionDataset):\n",
    "    num_targets = 10\n",
    "    def __init__(self,*args,angle =np.pi/3,N=50000,size=(7,7),\n",
    "                 train=True,dataseed=0, fp=None,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "\n",
    "        if fp is not None:\n",
    "            npz = fp\n",
    "            resx = np.linspace(-2., 1., 20)\n",
    "            self.data = np.zeros([N, 1, DIM, DIM])\n",
    "            for i in range(0, N):\n",
    "                #print(type(npz[i]), npz[i])\n",
    "                resy = (np.rint(npz[i]*9)).astype(int) // 2\n",
    "                resy = -resy + 9\n",
    "                for xi in range(0, DIM):\n",
    "                    #print(xi, resy.shape)\n",
    "                    self.data[i][0][resy[xi]][xi] = 1.\n",
    "\n",
    "        self.data = torch.Tensor(self.data)\n",
    "        #if not train: \n",
    "        #    dataseed += 1\n",
    "        #    N = int(0.2*N)\n",
    "        torch.manual_seed(dataseed)\n",
    "        angles = torch.ones(N)*angle # torch.rand(N)*2*np.pi\n",
    "        #self.data = torch.rand(N,1,*size)-.5\n",
    "        print(N, self.data.shape)\n",
    "        with torch.no_grad():\n",
    "            # Build affine matrices for random translation of each image\n",
    "            affineMatrices = torch.zeros(N,2,3)\n",
    "            affineMatrices[:,0,0] = angles.cos()\n",
    "            affineMatrices[:,1,1] = angles.cos()\n",
    "            affineMatrices[:,0,1] = angles.sin()\n",
    "            affineMatrices[:,1,0] = -angles.sin()\n",
    "            \n",
    "            flowgrid = F.affine_grid(affineMatrices, size = self.data.shape) # self.data.size()\n",
    "            self.data_rot = F.grid_sample(self.data, flowgrid)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx], self.data_rot[idx] # , self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def default_aug_layers(self):\n",
    "        return RandomRotateTranslate(0)# no translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500 torch.Size([8500, 1, 20, 20])\n",
      "2500 torch.Size([2500, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/ai/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "npz = np.load(\"ode10000x20.npz\")\n",
    "DIM  = 20 \n",
    "# dataset = CIFAR100\n",
    "dataset = FixedRot #AugRotMNIST__0\n",
    "# dataset = AugRotMNIST__0\n",
    "num_targets = dataset.num_targets\n",
    "\n",
    "batch_size = 150\n",
    "\n",
    "test_batch_size = 200 #1000 #150\n",
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "# if use_cuda:\n",
    "cuda_kwargs = {'num_workers': 1,\n",
    "               'pin_memory': True,\n",
    "               'shuffle': True}\n",
    "train_kwargs.update(cuda_kwargs)\n",
    "test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "ang_n = 45\n",
    "ang = np.pi/ang_n\n",
    "dataset1 = dataset('../data', fp=npz['arr_0'], N=8500, train=True, size=(DIM,DIM), angle=ang)\n",
    "dataset2 = dataset('../data', fp=npz['arr_0'], N=2500, train=False, size=(DIM,DIM), angle=ang)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x,y) in train_loader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 1, 20, 20]), torch.Size([100, 1, 20, 20]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dataset1.data[0].shape\n",
    "d = np.prod(s)\n",
    "# lc = Lconv_2(k=2,d=d, cin=1, cout=1, rank=50)\n",
    "rec = 8\n",
    "# model = Net(d,10,k=1,device=device,rec=rec).to(device)\n",
    "\n",
    "model = Net(d,10,k=1,device=device,rec=rec).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr = 5e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 1, 20, 20])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m t \u001b[39m=\u001b[39m trainer(model, device, optimizer, train_loader\u001b[39m=\u001b[39mtrain_loader, test_loader\u001b[39m=\u001b[39mtest_loader, \n\u001b[1;32m      2\u001b[0m             \u001b[39m#dataset_class=AugRotMNIST__0, #dataset, \u001b[39;00m\n\u001b[1;32m      3\u001b[0m             loss_func\u001b[39m=\u001b[39mF\u001b[39m.\u001b[39mmse_loss)\n\u001b[0;32m----> 4\u001b[0m t\u001b[39m.\u001b[39;49mfit(\u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[32], line 130\u001b[0m, in \u001b[0;36mtrainer.fit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m,epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    129\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 130\u001b[0m         r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(epoch)\n\u001b[1;32m    131\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mtrain loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [r[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mtrain acc\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [r[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[32], line 74\u001b[0m, in \u001b[0;36mtrainer.train\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m#             data, target = data.to(self.device), target.to(self.device)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 74\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(data)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     75\u001b[0m             output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(data)\n\u001b[1;32m     76\u001b[0m             loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_func(output, target)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inp):\n\u001b[1;32m     14\u001b[0m     \u001b[39mprint\u001b[39m(inp\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 15\u001b[0m     x,y \u001b[39m=\u001b[39m inp\n\u001b[1;32m     16\u001b[0m     \u001b[39m# x,y->(batch, channel, flat_d)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflat(x)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "t = trainer(model, device, optimizer, train_loader=train_loader, test_loader=test_loader, \n",
    "            #dataset_class=AugRotMNIST__0, #dataset, \n",
    "            loss_func=F.mse_loss)\n",
    "t.fit(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
